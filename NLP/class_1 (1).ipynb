{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1QCfl0-_5tl7"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finding phone no from a chat text"
      ],
      "metadata": {
        "id": "5ZFkwCE26jgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat1 = 'your no is: (123)-567-8912, abc@xyz.com'\n",
        "chat2 = 'your contact details are: 1235678912 email: abc@xyz.com'\n",
        "\n",
        "pattern = '\\(\\d{3}\\)-\\d{3}-\\d{4}|\\d{10}'\n",
        "matches = re.findall(pattern,chat1)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxWiTi-V5zR4",
        "outputId": "fc8644f7-3a74-4da0-c001-b6dfcead68c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['(123)-567-8912']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finding email id from a text"
      ],
      "metadata": {
        "id": "JDuQ08gK6pTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = '[a-z0-9_]*@[a-z]*\\.[a-z0-9]*'\n",
        "matches = re.findall(pattern,chat2)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axYs3_XZ6H3_",
        "outputId": "a8e4ddfa-4730-4698-b6d0-6d4808878e34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abc@xyz.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy Tokenization**"
      ],
      "metadata": {
        "id": "vF-qsq3lFD_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "5Sz4T2abFKQK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"Dr. Kiran loves Delhi, the capital city of India.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9SEewrUFPgG",
        "outputId": "bf46c715-9cc6-4395-d7a4-9566040b6411"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "Kiran\n",
            "loves\n",
            "Delhi\n",
            ",\n",
            "the\n",
            "capital\n",
            "city\n",
            "of\n",
            "India\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence tokenization and Segmentation**"
      ],
      "metadata": {
        "id": "oVjz6JNaFaWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dapeX2-0FwTE",
        "outputId": "b2f0a899-aad6-4c5e-cec0-f40825efc561"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x78120086b810>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Soham loves Indian cuisines. Tom on the other hand likes continental delicacies\")\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEiDq1-dFcXO",
        "outputId": "4c438c02-581a-4bd6-874c-0fbb4609e00e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soham loves Indian cuisines.\n",
            "Tom on the other hand likes continental delicacies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Morphological parsing using Spacy**"
      ],
      "metadata": {
        "id": "LSiIs4r51hnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Analyze a sentence\n",
        "doc = nlp(\"The cats are running quickly.\")\n",
        "\n",
        "# Iterate over tokens\n",
        "for token in doc:\n",
        "    print(token.text, \"|\", token.lemma_, \"|\", token.pos_ ,\"|\", token.morph )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do9FFnGt0jua",
        "outputId": "6c3ae90c-33a6-44c4-c0ba-641def41b1ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The | the | DET | Definite=Def|PronType=Art\n",
            "cats | cat | NOUN | Number=Plur\n",
            "are | be | AUX | Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "running | run | VERB | Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "quickly | quickly | ADV | \n",
            ". | . | PUNCT | PunctType=Peri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yki4PlzYsfSq"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS Tags**"
      ],
      "metadata": {
        "id": "tXVDpqo01VPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Tom flew to Canada yesterday. He carried a laptop with him\")\n",
        "for token in doc:\n",
        "  print(token,\"|\",token.pos_ ,\"|\", spacy.explain(token.pos_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CC8ZBC4zX4A",
        "outputId": "49bf9b80-2aa4-4d82-9373-b3842424c591"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom | PROPN | proper noun\n",
            "flew | VERB | verb\n",
            "to | ADP | adposition\n",
            "Canada | PROPN | proper noun\n",
            "yesterday | NOUN | noun\n",
            ". | PUNCT | punctuation\n",
            "He | PRON | pronoun\n",
            "carried | VERB | verb\n",
            "a | DET | determiner\n",
            "laptop | NOUN | noun\n",
            "with | ADP | adposition\n",
            "him | PRON | pronoun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Wow! Dr. Steve made $265 million on the very first day\")\n",
        "for token in doc:\n",
        "  print(token,\"|\",token.pos_,\"|\",spacy.explain(token.pos_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEpzQl3Pz3-F",
        "outputId": "779b925f-3806-4349-a875-d78f89444a8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow | INTJ | interjection\n",
            "! | PUNCT | punctuation\n",
            "Dr. | PROPN | proper noun\n",
            "Steve | PROPN | proper noun\n",
            "made | VERB | verb\n",
            "$ | SYM | symbol\n",
            "265 | NUM | numeral\n",
            "million | NUM | numeral\n",
            "on | ADP | adposition\n",
            "the | DET | determiner\n",
            "very | ADV | adverb\n",
            "first | ADJ | adjective\n",
            "day | NOUN | noun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tag**"
      ],
      "metadata": {
        "id": "dNeFUluP1Q6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Wow! Dr. Steve made $265 million  on the very first day\")\n",
        "for token in doc:\n",
        "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoqYsh5A0ftW",
        "outputId": "4a96a254-2ff0-48f7-9547-de8327f22a49"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow  |  INTJ  |  interjection  |  UH  |  interjection\n",
            "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
            "Dr.  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            "Steve  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            "made  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
            "$  |  SYM  |  symbol  |  $  |  symbol, currency\n",
            "265  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "million  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "   |  SPACE  |  space  |  _SP  |  whitespace\n",
            "on  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
            "the  |  DET  |  determiner  |  DT  |  determiner\n",
            "very  |  ADV  |  adverb  |  RB  |  adverb\n",
            "first  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
            "day  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porter Stemmer"
      ],
      "metadata": {
        "id": "DjPNagoeBcGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# List of words to stem\n",
        "words = [\"running\", \"runner\", \"easily\", \"flying\", \"studies\", \"happiness\"]\n",
        "\n",
        "# Stem each word\n",
        "for word in words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    print(f\"Original: {word} -> Stemmed: {stemmed_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF77XBRjBI_3",
        "outputId": "050a6f16-c422-488f-d831-44a703c8609e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: running -> Stemmed: run\n",
            "Original: runner -> Stemmed: runner\n",
            "Original: easily -> Stemmed: easili\n",
            "Original: flying -> Stemmed: fli\n",
            "Original: studies -> Stemmed: studi\n",
            "Original: happiness -> Stemmed: happi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball stemmer"
      ],
      "metadata": {
        "id": "ZSE5QAMWBw6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Initialize Snowball Stemmer for English\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "words = [\"running\", \"better\", \"flies\", \"happiness\", \"studying\"]\n",
        "for word in words:\n",
        "    print(f\"Original: {word} -> Stemmed: {stemmer.stem(word)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUdrVdDYBmN7",
        "outputId": "a3b057a7-e950-4184-abdb-a3522c8e8c54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: running -> Stemmed: run\n",
            "Original: better -> Stemmed: better\n",
            "Original: flies -> Stemmed: fli\n",
            "Original: happiness -> Stemmed: happi\n",
            "Original: studying -> Stemmed: studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "e1pnBfCtRKH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "words = [\"running\", \"better\", \"flies\", \"happiness\", \"studying\"]\n",
        "for word in words:\n",
        "    doc = nlp(word)\n",
        "    for token in doc:\n",
        "        print(f\"Original: {token.text} -> Lemmatized: {token.lemma_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Poas5734B0aL",
        "outputId": "481a731b-d4c9-4d3b-9d68-a8a99f9b1bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: running -> Lemmatized: run\n",
            "Original: better -> Lemmatized: well\n",
            "Original: flies -> Lemmatized: fly\n",
            "Original: happiness -> Lemmatized: happiness\n",
            "Original: studying -> Lemmatized: study\n"
          ]
        }
      ]
    }
  ]
}